{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec을 활용한 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])\n",
    "\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300   # 워드 벡터 특징값 수\n",
    "min_word_count = 40  # 단어에 대한 최소 빈도 수\n",
    "num_workers = 4      # 프로세스 개수\n",
    "context = 10         # 컨텍스트 윈도우 크기 \n",
    "downsampling = 1e-3  # 다운 샘플링 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_features: 각 단어에 대해 임베딩된 벡터의 차원을 정한다\n",
    "- min_word_count: 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다\n",
    "- num_workers: 모델 학습 시 학습을 위한 프로세스 개수를 지정한다.\n",
    "- context: word2vec을 수행하기 위한 컨텍스트 윈도우 크기를 지정한다.\n",
    "- downsampling: word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링 비율을 지정한다. 보통 0.001이 좋은 성능을 낸다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from smart-open>=1.7.0->gensim) (1.9.123)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: bz2file in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied: requests in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.123 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.123)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from botocore<1.13.0,>=1.12.123->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\zlslsp54\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from botocore<1.13.0,>=1.12.123->boto3->smart-open>=1.7.0->gensim) (0.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "진행 상황 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 16:36:32,534 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-05-09 16:36:32,547 : INFO : collecting all words and their counts\n",
      "2019-05-09 16:36:32,548 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 16:36:32,924 : INFO : PROGRESS: at sentence #10000, processed 1221909 words, keeping 55812 word types\n",
      "2019-05-09 16:36:33,290 : INFO : PROGRESS: at sentence #20000, processed 2429834 words, keeping 74689 word types\n",
      "2019-05-09 16:36:33,471 : INFO : collected 82107 word types from a corpus of 3029622 raw words and 25000 sentences\n",
      "2019-05-09 16:36:33,472 : INFO : Loading a fresh vocabulary\n",
      "2019-05-09 16:36:33,642 : INFO : effective_min_count=40 retains 8132 unique words (9% of original 82107, drops 73975)\n",
      "2019-05-09 16:36:33,643 : INFO : effective_min_count=40 leaves 2648221 word corpus (87% of original 3029622, drops 381401)\n",
      "2019-05-09 16:36:33,682 : INFO : deleting the raw counts dictionary of 82107 items\n",
      "2019-05-09 16:36:33,686 : INFO : sample=0.001 downsamples 29 most-common words\n",
      "2019-05-09 16:36:33,687 : INFO : downsampling leaves estimated 2490621 word corpus (94.0% of prior 2648221)\n",
      "2019-05-09 16:36:33,729 : INFO : estimated required memory for 8132 words and 300 dimensions: 23582800 bytes\n",
      "2019-05-09 16:36:33,730 : INFO : resetting layer weights\n",
      "2019-05-09 16:36:33,912 : INFO : training model with 4 workers on 8132 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-09 16:36:34,932 : INFO : EPOCH 1 - PROGRESS: at 17.95% examples, 448487 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:35,937 : INFO : EPOCH 1 - PROGRESS: at 34.90% examples, 434826 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:36,958 : INFO : EPOCH 1 - PROGRESS: at 53.08% examples, 438603 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:37,962 : INFO : EPOCH 1 - PROGRESS: at 72.84% examples, 450854 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-09 16:36:38,982 : INFO : EPOCH 1 - PROGRESS: at 92.32% examples, 454951 words/s, in_qsize 8, out_qsize 1\n",
      "2019-05-09 16:36:39,349 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-09 16:36:39,361 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-09 16:36:39,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-09 16:36:39,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-09 16:36:39,391 : INFO : EPOCH - 1 : training on 3029622 raw words (2490488 effective words) took 5.5s, 455051 effective words/s\n",
      "2019-05-09 16:36:40,403 : INFO : EPOCH 2 - PROGRESS: at 17.62% examples, 444942 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:41,440 : INFO : EPOCH 2 - PROGRESS: at 36.17% examples, 446396 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:42,444 : INFO : EPOCH 2 - PROGRESS: at 54.34% examples, 448871 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:43,449 : INFO : EPOCH 2 - PROGRESS: at 72.48% examples, 448267 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:44,487 : INFO : EPOCH 2 - PROGRESS: at 89.96% examples, 441770 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:45,043 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-09 16:36:45,059 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-09 16:36:45,062 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-09 16:36:45,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-09 16:36:45,090 : INFO : EPOCH - 2 : training on 3029622 raw words (2490967 effective words) took 5.7s, 437795 effective words/s\n",
      "2019-05-09 16:36:46,115 : INFO : EPOCH 3 - PROGRESS: at 17.62% examples, 438735 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:47,123 : INFO : EPOCH 3 - PROGRESS: at 36.74% examples, 457403 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:48,177 : INFO : EPOCH 3 - PROGRESS: at 54.65% examples, 446011 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:49,189 : INFO : EPOCH 3 - PROGRESS: at 71.76% examples, 439459 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:50,199 : INFO : EPOCH 3 - PROGRESS: at 89.65% examples, 438899 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:50,658 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-09 16:36:50,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-09 16:36:50,696 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-09 16:36:50,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-09 16:36:50,714 : INFO : EPOCH - 3 : training on 3029622 raw words (2490659 effective words) took 5.6s, 443277 effective words/s\n",
      "2019-05-09 16:36:51,724 : INFO : EPOCH 4 - PROGRESS: at 17.98% examples, 453324 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:52,744 : INFO : EPOCH 4 - PROGRESS: at 38.47% examples, 478354 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:53,764 : INFO : EPOCH 4 - PROGRESS: at 57.27% examples, 472951 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:54,792 : INFO : EPOCH 4 - PROGRESS: at 78.10% examples, 479916 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:55,802 : INFO : EPOCH 4 - PROGRESS: at 97.63% examples, 479183 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:55,897 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-09 16:36:55,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-09 16:36:55,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-09 16:36:55,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-09 16:36:55,933 : INFO : EPOCH - 4 : training on 3029622 raw words (2491046 effective words) took 5.2s, 477958 effective words/s\n",
      "2019-05-09 16:36:56,971 : INFO : EPOCH 5 - PROGRESS: at 18.28% examples, 449399 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:36:57,980 : INFO : EPOCH 5 - PROGRESS: at 37.12% examples, 458499 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:36:58,983 : INFO : EPOCH 5 - PROGRESS: at 55.32% examples, 456994 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:37:00,033 : INFO : EPOCH 5 - PROGRESS: at 72.54% examples, 443672 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-09 16:37:01,046 : INFO : EPOCH 5 - PROGRESS: at 90.60% examples, 443348 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-09 16:37:01,625 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-09 16:37:01,626 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-09 16:37:01,658 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-09 16:37:01,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-09 16:37:01,667 : INFO : EPOCH - 5 : training on 3029622 raw words (2490898 effective words) took 5.7s, 434947 effective words/s\n",
      "2019-05-09 16:37:01,668 : INFO : training on a 15148110 raw words (12454058 effective words) took 27.8s, 448728 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers=num_workers,\n",
    "                         size=num_features,\n",
    "                         min_count = min_word_count,\n",
    "                         window = context,\n",
    "                         sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장 해놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 16:41:03,924 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-05-09 16:41:03,926 : INFO : not storing attribute vectors_norm\n",
      "2019-05-09 16:41:03,929 : INFO : not storing attribute cum_table\n",
      "2019-05-09 16:41:04,246 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            \n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 문장에 특징값을 만들 수 있는 함수를 구현했다면 이제 앞에서 정의한 함수를 사용해 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zlslsp54\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만들어진 데이터를 가지고 학습 데이터와 검증 데이터를 나눠보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size = TEST_SPLIT, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 word2vec 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 선언 및 학습\n",
    "\n",
    "로지스틱 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zlslsp54\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight = 'balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 데이터셋을 이용한 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866400\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF와 별로 차이가 나지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 캐글 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
